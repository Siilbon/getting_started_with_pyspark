{\rtf1\ansi\ansicpg1252\cocoartf1561\cocoasubrtf100
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 import pandas as pd\
import matplotlib.pyplot as plt\
import seaborn as sns\
\
\
from pyspark.sql import SparkSession\
\
spark = (\
from pyspark.sql import SparkSession\
\
spark = (\
    SparkSession\
    .builder\
    .appName('testing')\
    .master('local[2]')\
    .getOrCreate()\
)\
\
import pyspark.sql.functions as f\
import pyspark.sql.types as t\
\
df = (\
    spark\
    .read\
    .format('csv')\
    .option('header', 'true')\
    .option('inferSchema', 'true')\
    .load('taxi.csv')\
)\
\
df = df.select(*[f.col(c).alias('_'.join(c.lower().split(' '))) for c in df.columns])\
\
df.limit(100).toPandas()\
\
df.groupby('taxi_id').count().count()\
\
df.groupby('trip_id').count().count()\
\
\
trips_per_taxi = (\
    df\
    .where((f.col('payment_type') == 'Credit Card') | (f.col('payment_type') == 'Cash'))\
    .groupby('taxi_id', 'payment_type')\
    .count()\
    .toPandas()\
)\
\
plt.figure()\
sns.distplot(\
    trips_per_taxi.loc[trips_per_taxi['payment_type']=='Credit Card', 'count']\
).set_title('Trips Per Taxi Paid By Credit Card')\
\
\
plt.figure()\
sns.distplot(\
    trips_per_taxi.loc[trips_per_taxi['payment_type']=='Cash', 'count']\
).set_title('Trips Per Taxi Paid By Cash')\
\
df.groupby('taxi_id').count().orderBy('count', ascending=True).limit(10).toPandas()\
\
df.groupby('taxi_id').count().orderBy('count', ascending=False).limit(10).toPandas()\
\
counts = df.groupby('taxi_id').count().toPandas()\
\
sns.distplot(counts['count']).set_title('Trips Per Taxi')\
\
\
date_format = 'MM/dd/yyyy hh:mm:ss a'\
df = (\
    df\
    .withColumn('start_time', f.to_timestamp(f.col('trip_start_timestamp'), date_format))\
    .withColumn('end_time', f.to_timestamp(f.col('trip_end_timestamp'), date_format))\
)\
\
(\
    df\
    .groupby(f.year('start_time').alias('year'))\
    .count()\
    .orderBy('year')\
    .toPandas()\
)\
\
df.agg(f.min(f.col('start_time')), f.max(f.col('end_time'))).toPandas()\
\
\
features = df.select(\
    f.col('pickup_census_tract').cast(t.DoubleType()).alias('census_tract'),\
    f.month('start_time').alias('month'),\
    f.year('start_time').alias('year'),\
    f.dayofweek('start_time').alias('day_of_week'),\
    f.hour('start_time').alias('hour'),\
    'trip_total',\
    'trip_seconds',\
    'trip_miles',\
    f.col('tips').alias('label'),\
).where(f.col('payment_type') == 'Credit Card')\
\
features.limit(10).toPandas()\
\
\
features = features.where('label is not null')\
\
feature_cols = [c for c in features.columns if c != 'label']\
\
\
print(feature_cols)\
\
\
from pyspark.ml.feature import Imputer, VectorAssembler\
from pyspark.ml.regression import RandomForestRegressor, GBTRegressor\
from pyspark.ml import Pipeline\
from pyspark.ml.evaluation import RegressionEvaluator\
\
\
features.agg(*[f.sum(f.col(c).isNull().cast(t.IntegerType())).alias(c) for c in features.columns]).toPandas()\
\
features = features.select([f.col(c).cast(t.DoubleType()) for c in features.columns])\
\
(train, test) = features.randomSplit([0.7, 0.3])\
train.cache()\
test.cache()\
\
\
def feature_importances(train, model, n=20):\
    """\
    Print feature name and importance score for top n features\
    Parameters\
    ----------\
    train: spark df\
        must have 'feature' column containing the spark vector used as training data\
    model: sparkML transformer\
        must be a model type that has the `featureImportances` attribute\
    n: int (defaults to 20)\
        number of features to return\
    """\
    feature_types = train.schema['features'].metadata['ml_attr']['attrs']\
    feature_col_names = []\
    if 'nominal' in feature_types.keys():\
        feature_col_names.extend([i['name'] for i in train.schema['features'].metadata['ml_attr']['attrs']['nominal']])\
    if 'numeric' in feature_types.keys():\
        feature_col_names.extend([j['name'] for j in train.schema['features'].metadata['ml_attr']['attrs']['numeric']])\
\
    importances = sorted(zip(model.featureImportances.values, model.featureImportances.indices), reverse=True)\
\
    for val, i in importances[0:n]:\
        print('\{0\}: \{1:.3f\}'.format(feature_col_names[i], val))\
\
\
\
pipeline = rf_pipeline\
\
model = pipeline.fit(train)\
train_predictions = model.transform(train)\
predictions = model.transform(test)\
\
evaluator = RegressionEvaluator(\
    labelCol="label",\
    predictionCol="prediction",\
    metricName="rmse",\
)\
rmse = evaluator.evaluate(train_predictions)\
print("RMSE on train data = \{\}".format(rmse))\
\
rmse = evaluator.evaluate(predictions)\
print("RMSE on test data = \{\}".format(rmse))\
\
print('Most important features:')\
feature_importances(predictions, model.stages[2])\
\
\
to_plot = (\
    predictions\
    .select('prediction', 'label')\
    .sample(withReplacement=False, fraction=.1)\
    .toPandas()\
)\
\
plt.figure()\
sns.distplot(to_plot['prediction'] - to_plot['label'])\
\
\
plt.figure()\
sns.distplot(to_plot['label'])\
\
\
plt.figure()\
sns.distplot(to_plot['prediction'])}